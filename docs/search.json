[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "AI businesses built to be profit margin plays suck\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nJay Lowe\n\n\n\n\n\n\n\n\n\n\n\n\nEnergized AI Hypotheses - [PART 5]\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nSep 7, 2024\n\n\nJay Lowe\n\n\n\n\n\n\n\n\n\n\n\n\nBy the Power of Regulation - [PART 4]\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nAug 31, 2024\n\n\nJay Lowe\n\n\n\n\n\n\n\n\n\n\n\n\nHow the Heck Do We Transmit Energy: [PART 3]\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nAug 24, 2024\n\n\nJay Lowe\n\n\n\n\n\n\n\n\n\n\n\n\nDAMS DAMS DAMS: [PART 2]\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nAug 17, 2024\n\n\nJay Lowe\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Energy Sub-Sectors: [PART 1]\n\n\n\n\n\n\nenergy\n\n\nai\n\n\nseries\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nJay Lowe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi and welcome to the ogjaylowe blog!\nI first created this as a way to organize and share my learnings with the wider world. Check out my first post for more details on what I‚Äôm aiming to achieve with my writing.\nRecently, I‚Äôm moving away from some of my older content and as a result have temporarily removed all of my previous AI content as I focus on a potentially bigger issue‚Äìhow we provide the energy needed to train models!\nI‚Äôm currently a Solutions Engineer at Bifrost.ai and am deeply embedded into the synthetic data space (pun intended)."
  },
  {
    "objectID": "posts/template/index.html",
    "href": "posts/template/index.html",
    "title": "Template Post",
    "section": "",
    "text": "Template plot containing code blocks, markdown features, and plots."
  },
  {
    "objectID": "posts/template/index.html#polar-axis",
    "href": "posts/template/index.html#polar-axis",
    "title": "Template Post",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure¬†1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 4 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure¬†1: A line plot on a polar axis\n\n\n\n\n\n# importing module\nimport matplotlib.pyplot as plt\n \n# assigning x and y coordinates\nx = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\ny = []\n \nfor i in range(len(x)):\n    y.append(max(0, x[i]))\n \n# depicting the visualization\nplt.plot(x, y, color='green')\nplt.xlabel('X')\nplt.ylabel('Y')\n \n# square plot\nplt.axis('square')\n \n# displaying the title\nplt.title('ReLU Function')\n\nText(0.5, 1.0, 'ReLU Function')"
  },
  {
    "objectID": "posts/The Sovereign Tech Dad/index.html",
    "href": "posts/The Sovereign Tech Dad/index.html",
    "title": "The Sovereign Tech Dad",
    "section": "",
    "text": "Learn why this blog exists, how to get value from it, and what it means to be a sovereign tech dad."
  },
  {
    "objectID": "posts/The Sovereign Tech Dad/index.html#how-to-get-value",
    "href": "posts/The Sovereign Tech Dad/index.html#how-to-get-value",
    "title": "The Sovereign Tech Dad",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "posts/The Sovereign Tech Dad/index.html#why-sovereign-tech-dad",
    "href": "posts/The Sovereign Tech Dad/index.html#why-sovereign-tech-dad",
    "title": "The Sovereign Tech Dad",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "posts/The Sovereign Tech Dad/index.html#final-thoughts-and-a-call-to-action",
    "href": "posts/The Sovereign Tech Dad/index.html#final-thoughts-and-a-call-to-action",
    "title": "The Sovereign Tech Dad",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "Jay Lowe's Blog",
    "section": "",
    "text": "achieve new rev streams\nshowcase knowledge and become an industry leader in ML / Fatherhood\nempower followers to become better dads and developers"
  },
  {
    "objectID": "ideas.html#tags",
    "href": "ideas.html#tags",
    "title": "Jay Lowe's Blog",
    "section": "Tags",
    "text": "Tags\n\nRant\nFatherhood\nSovereignty\nHow To\nTech Stack X\nEssay / Oped\nBook / Art / Poem Review"
  },
  {
    "objectID": "ideas.html#series-ideas",
    "href": "ideas.html#series-ideas",
    "title": "Jay Lowe's Blog",
    "section": "Series Ideas",
    "text": "Series Ideas\n\n‚ÄúHow To‚Äù focused on doing what I‚Äôm learning. Typically in two parts, (1) being a learning series on my blog about the apriori concepts following other works and (2) being a cross post to Roboflow in which I put the learning into action.\n\nEX: recreating the monte carlo portion of a RL learning with CV included\nEX: how to create an openAI gym env and then making one with CV\noften includes a series in which problem statements get resolved further in each post\n\nEX: start with problem statement 1, the work done to validate that, and a link to the next post which will further refine the problem statement and a hypothesis to what that will be (in next post respond to hypothesis)\nEX: could also be a hypothesis statement on the future of events if an oped type piece\n\ninclude notes and remarks on interview prep and how readers could use these learned lessons from a cut tech vet\n\nProblems I would like to see solved by others?\nGeneral lessons learned at certain milestones\n\nfather hood and homeschooling major milestones (birthdays, moves, ‚Äúfirst of‚Äù-type events, pregnancy, etc)\ndeveloper milestones (PR, new initative, new roadmap, new framework learned, new tech adopted into a stack, etc.)\nblogging and social media milestones (X followers / email sign ups, Xth post, certain engagement with a post learnings, any real ship)\nfitness milestones (X% bodyfat or weight, new PB, practicing a new diet or excercise for a week such as HIIP or hydration tablets)\nresearch learnings (read X papers and grokked something important, made a new working prototype that helped me understand something like a Q agent better, thoughts on an emerging movement)\n\nChallenging society and my industry?"
  },
  {
    "objectID": "ideas.html#distribution-targets",
    "href": "ideas.html#distribution-targets",
    "title": "Jay Lowe's Blog",
    "section": "Distribution targets",
    "text": "Distribution targets\n\nmy blog\nRoboflow blog\nhackernews and other outlets?"
  },
  {
    "objectID": "ideas.html#parenting-topics",
    "href": "ideas.html#parenting-topics",
    "title": "Jay Lowe's Blog",
    "section": "Parenting Topics",
    "text": "Parenting Topics\n\nmusic (piano, electronic music)\nreading / writing (chalk board, making a website)\nprogramming (websites, ML interviews, SWE interviews)\npainting (watercolor)\ncooking (french, creol, foreign)\nsports (bjj, yoga, tennis, kids football)\noutdoor (hikes, camping)\nscience (temperature, water cycles, geology, space)\npersonal brand (watch the masterclass on personal brand and making videos)\nrandom (dogs, gardening)"
  },
  {
    "objectID": "ideas.html#marketing",
    "href": "ideas.html#marketing",
    "title": "Jay Lowe's Blog",
    "section": "Marketing",
    "text": "Marketing\nIndustry: AI and SWE education Theme: the funny fit dad engineer that helps you get to where you need to be\nContent focused on breaking down how deep mind built what they did, then replicating it in my own projects with Roboflow\nHighly targeted niche (HTN) Demographic - Age = 25-40 - Gender = Male - Location = Tech hubs such as Bay Area - Interest = ML technology and career transitions\nCash angle? - Turn everything on camera into a rev stream. Clothing, software, hardware, OS, everything - parenting things can turn into rev streams as well"
  },
  {
    "objectID": "ideas.html#potential-partners-to-recpricate-with",
    "href": "ideas.html#potential-partners-to-recpricate-with",
    "title": "Jay Lowe's Blog",
    "section": "potential partners to recpricate with",
    "text": "potential partners to recpricate with\n\ntech products such as:\n\nNotion\nGrammarly\nQuarto\nGithub pages\n\nthat schooling website and home schooling communities\nAjax and the wider health community\nthe soverign community\nthe success and achievement of others in my community!\n\nblog creation notes: https://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#first-simple-changes"
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "",
    "text": "TODO: when to be greedy, exploitation vs exploration explained\nTODO: action value methods?"
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#meet-the-agent",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#meet-the-agent",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Meet the agent",
    "text": "Meet the agent\nTODO - meet agent Rae?\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while getting jacked up on blocks falling would produce low reward (as they rile her up)."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#how-to-get-value",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#how-to-get-value",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#why-sovereign-tech-dad",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#why-sovereign-tech-dad",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#final-thoughts-and-a-call-to-action",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#final-thoughts-and-a-call-to-action",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#what-does-reinforcement-learning-do",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#what-does-reinforcement-learning-do",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "What does reinforcement learning do?",
    "text": "What does reinforcement learning do?\nGiven an environment and a set of actions that can be performed in that environment, an RL algorithm learns how to maximize rewards within the context of a given goal.\nHearby, the RL algorithm performing actions shall be known as the agent.\nTODO - meet agent Rae?\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to bed‚Äìthe goal set for her in this bedroom (dictated by time of day and me the father) will determine which actions lead to best fufilling that goal."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "posts/Bandit_1/index.html",
    "href": "posts/Bandit_1/index.html",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "",
    "text": "evaluation methods explained"
  },
  {
    "objectID": "posts/Bandit_1/index.html#why-bandits",
    "href": "posts/Bandit_1/index.html#why-bandits",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why bandits?",
    "text": "Why bandits?\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull."
  },
  {
    "objectID": "posts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "href": "posts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Providing bandit actions a value",
    "text": "Providing bandit actions a value\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to exploit that to our advantage.\nLet‚Äôs create some perfect ground truth values for a three armed bandit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n\n\n\n\nUnfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\nWe can‚Äôt provide a static ground truth value for a bandit arm or else a greedy RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.\n\nA better action-value method\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n\n\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.\nLet‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that as 2000 total action ‚Äútime steps‚Äù.\n\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function."
  },
  {
    "objectID": "posts/Bandit_1/index.html#meet-the-agent",
    "href": "posts/Bandit_1/index.html#meet-the-agent",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet the agent",
    "text": "Meet the agent\nTODO - meet agent Rae?\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while getting jacked up on blocks falling would produce low reward (as they rile her up)."
  },
  {
    "objectID": "posts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "posts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "posts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "posts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "posts/Bandit_1/index.html#actions-have-results",
    "href": "posts/Bandit_1/index.html#actions-have-results",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "posts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "posts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "posts/Bandit_1/index.html#planning-for-future-actions",
    "href": "posts/Bandit_1/index.html#planning-for-future-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "posts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "posts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL."
  },
  {
    "objectID": "posts/Bandit_1/index.html#how-to-get-value",
    "href": "posts/Bandit_1/index.html#how-to-get-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "posts/Bandit_1/index.html#why-sovereign-tech-dad",
    "href": "posts/Bandit_1/index.html#why-sovereign-tech-dad",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "posts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "href": "posts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "posts/Bandit_1/index.html#defining-a-bandit",
    "href": "posts/Bandit_1/index.html#defining-a-bandit",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Defining a bandit",
    "text": "Defining a bandit\nA bandit represents an environment, a set of rewards, and a set of actions.\nFor example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1."
  },
  {
    "objectID": "posts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "href": "posts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Greedy and nongreedy actions",
    "text": "Greedy and nongreedy actions\nAn agent will take a greedy action if it has been told to exploit its environment and choose the highest reward possible.\nIt will choose the nongreedy action during exploration in an attempt to better estimate the reward value an action will provide.\nOne should exploit once the optimal policy and value functions have been determined, as that will lead to maximum rewards.\nIdentifying the right ratio of exploitaton to exploration and the exploration decay rate plays a critical role in a succesfully converging an RL agent to optimal performance."
  },
  {
    "objectID": "posts/Bandit_1/index.html#meet-Œµ",
    "href": "posts/Bandit_1/index.html#meet-Œµ",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet Œµ",
    "text": "Meet Œµ\nThe ratio of when the agent chooses to exploit or explore commonly gets denoted with the greek notation of epsilon Œµ.\nIn general, exploration should be occur drastically less then exploit, as the agent should be focused on finding the maximum reward, so usually we set Œµ to equal to .1 or less.\n\nWhy exploration works\nIf you were to give an agent infinite resources and an Œµ value greater then zero, as it approaches the limit of infinity, it will exhaust all possible actions and thereby discover the ground truth values of a bandit.\nAgents do not have infinite resources but give them enough time and they will approach the limit close enough to accurately estimate the ground truth value to a certain acceptable degree."
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "",
    "text": "Understand the energy market‚Äôs sub-sectors and how the market as a whole connects together"
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#defining-a-bandit",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#defining-a-bandit",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Defining a bandit",
    "text": "Defining a bandit\nA bandit represents an environment, a set of rewards, and a set of actions.\nFor example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1."
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-bandits",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-bandits",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Why bandits?",
    "text": "Why bandits?\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull."
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#providing-bandit-actions-a-value",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#providing-bandit-actions-a-value",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Providing bandit actions a value",
    "text": "Providing bandit actions a value\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to exploit that to our advantage.\nLet‚Äôs create some perfect ground truth values for a three armed bandit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n\n\n\n\n\n\n\n\nUnfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\nWe can‚Äôt provide a static ground truth value for a bandit arm or else a greedy RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.\n\nA better action-value method\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.\nLet‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that has 2000 total action ‚Äútime steps‚Äù.\n\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#actions-have-results",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#actions-have-results",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#planning-for-future-actions",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#planning-for-future-actions",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "posts/Learn_RL_Fundamentals_and_Vocab_in_Five_Minutes/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Learn RL Fundamentals and Vocab in Five Minutes",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL.\nTODO - Break into three posts"
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "",
    "text": "Learn all the reinforcement learning (RL) fundamental concepts and terminology such as reward and value function in five minutes.\n\nTo help us better understand RL, I will be pairing technical writing with a simple analogy comparing an RL algorithm to that of a three year old.\nMy daughter was three at the time of this writing and it was fun explaining RL concepts to my non-technical wife as the kid jumps around and yells, so hopefully it helps you as well."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#meet-the-agent",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#meet-the-agent",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "Meet the agent",
    "text": "Meet the agent\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while violently jumping on the bed would produce low reward (as goal == going to bed). If she wanted to play, which would probably be the case without intervention, then jumping on the bed would produce a high reward."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#conclusion",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#conclusion",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "Conclusion",
    "text": "Conclusion\nTest yourself with the following questions: - ‚ÄúCan everything truly be solved with by RL? If yes, list out some situations that would be impossible for an RL agent to work in.‚Äù - ‚ÄúDefine an environment, set of actions, an achevialbe goal that can be reached through those actions, and a relevant reward system to measure progress on that goal.‚Äù\nTweet your answer to me at @ogjaylowe so we can have a chat about it! Would love to discuss."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-to-read-next",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-to-read-next",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "What to read next",
    "text": "What to read next\nGot a hang for the fundamentals of RL and looking for more to read? Check out my post on additional complexities and components of an RL algorithm , or if you want to start coding, read about creating a simple bandit."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "",
    "text": "Learn how a reinforcement learning (RF) algorithm uses policy and value functions to balance short vs long term rewards.\n\nNew to the blog? Start at Learn RL Fundamentals in Five Minutes Level 1."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#actions-have-results",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#actions-have-results",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#planning-for-future-actions",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#planning-for-future-actions",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL!"
  },
  {
    "objectID": "zzz_templates/template/index.html",
    "href": "zzz_templates/template/index.html",
    "title": "Template Post",
    "section": "",
    "text": "Template plot containing code blocks, markdown features, and plots."
  },
  {
    "objectID": "zzz_templates/template/index.html#polar-axis",
    "href": "zzz_templates/template/index.html#polar-axis",
    "title": "Template Post",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure¬†1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 4 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis\n\n\n\n\n\n\n# importing module\nimport matplotlib.pyplot as plt\n \n# assigning x and y coordinates\nx = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\ny = []\n \nfor i in range(len(x)):\n    y.append(max(0, x[i]))\n \n# depicting the visualization\nplt.plot(x, y, color='green')\nplt.xlabel('X')\nplt.ylabel('Y')\n \n# square plot\nplt.axis('square')\n \n# displaying the title\nplt.title('ReLU Function')\n\nText(0.5, 1.0, 'ReLU Function')"
  },
  {
    "objectID": "zzz_templates/welcome/index.html",
    "href": "zzz_templates/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "drafts/the_big_one.html",
    "href": "drafts/the_big_one.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "",
    "text": "TODO: when to be greedy, exploitation vs exploration explained\nTODO: action value methods?"
  },
  {
    "objectID": "drafts/the_big_one.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "drafts/the_big_one.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "drafts/the_big_one.html#actions-have-results",
    "href": "drafts/the_big_one.html#actions-have-results",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "drafts/the_big_one.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "drafts/the_big_one.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "drafts/the_big_one.html#planning-for-future-actions",
    "href": "drafts/the_big_one.html#planning-for-future-actions",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "drafts/the_big_one.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "drafts/the_big_one.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL.\nTODO - Break into three posts"
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#conclusion",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#conclusion",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Conclusion",
    "text": "Conclusion\nEstablishing the proper policy and value functions provides context for reward.\nRewards can get complex, but finding the right balance of short and long term reward structures leads to the highest performing agents.\nTest yourself with the following questions: - ‚ÄúWhen should algorithms have a premium on short term rewards vs long term rewards‚Äù - ‚ÄúWhat enviroment factors should an engineer consider when creating dynamic value functions‚Äù\nTweet your answer to me at @ogjaylowe so we can have a chat about it! Would love to discuss."
  },
  {
    "objectID": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#what-to-read-next",
    "href": "posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#what-to-read-next",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "What to read next",
    "text": "What to read next\nReady to start coding?\nUp next read about creating a simple bandit."
  },
  {
    "objectID": "drafts/Bandit_1/index.html",
    "href": "drafts/Bandit_1/index.html",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "",
    "text": "evaluation methods explained"
  },
  {
    "objectID": "drafts/Bandit_1/index.html#defining-a-bandit",
    "href": "drafts/Bandit_1/index.html#defining-a-bandit",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Defining a bandit",
    "text": "Defining a bandit\nA bandit represents an environment, a set of rewards, and a set of actions.\nFor example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#why-bandits",
    "href": "drafts/Bandit_1/index.html#why-bandits",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why bandits?",
    "text": "Why bandits?\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "href": "drafts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Providing bandit actions a value",
    "text": "Providing bandit actions a value\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to exploit that to our advantage.\nLet‚Äôs create some perfect ground truth values for a three armed bandit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n\n\n\n\n\n\n\n\nUnfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\nWe can‚Äôt provide a static ground truth value for a bandit arm or else a greedy RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.\n\nA better action-value method\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.\nLet‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that as 2000 total action ‚Äútime steps‚Äù.\n\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "href": "drafts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Greedy and nongreedy actions",
    "text": "Greedy and nongreedy actions\nAn agent will take a greedy action if it has been told to exploit its environment and choose the highest reward possible.\nIt will choose the nongreedy action during exploration in an attempt to better estimate the reward value an action will provide.\nOne should exploit once the optimal policy and value functions have been determined, as that will lead to maximum rewards.\nIdentifying the right ratio of exploitaton to exploration and the exploration decay rate plays a critical role in a succesfully converging an RL agent to optimal performance."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#meet-Œµ",
    "href": "drafts/Bandit_1/index.html#meet-Œµ",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet Œµ",
    "text": "Meet Œµ\nThe ratio of when the agent chooses to exploit or explore commonly gets denoted with the greek notation of epsilon Œµ.\nIn general, exploration should be occur drastically less then exploit, as the agent should be focused on finding the maximum reward, so usually we set Œµ to equal to .1 or less.\n\nWhy exploration works\nIf you were to give an agent infinite resources and an Œµ value greater then zero, as it approaches the limit of infinity, it will exhaust all possible actions and thereby discover the ground truth values of a bandit.\nAgents do not have infinite resources but give them enough time and they will approach the limit close enough to accurately estimate the ground truth value to a certain acceptable degree."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#meet-the-agent",
    "href": "drafts/Bandit_1/index.html#meet-the-agent",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet the agent",
    "text": "Meet the agent\nTODO - meet agent Rae?\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while getting jacked up on blocks falling would produce low reward (as they rile her up)."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "drafts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "drafts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#actions-have-results",
    "href": "drafts/Bandit_1/index.html#actions-have-results",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "drafts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#planning-for-future-actions",
    "href": "drafts/Bandit_1/index.html#planning-for-future-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "drafts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#how-to-get-value",
    "href": "drafts/Bandit_1/index.html#how-to-get-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#why-sovereign-tech-dad",
    "href": "drafts/Bandit_1/index.html#why-sovereign-tech-dad",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "drafts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "href": "drafts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About",
    "section": "",
    "text": "Hi and welcome to the ogjaylowe blog!\nI first created this as a way to organize and share my learnings with the wider world. Check out my first post for more details on what I‚Äôm aiming to achieve with my writing.\nRecently, I‚Äôm moving away from some of my older content and as a result have temporarily removed all of my previous AI content as I focus on a potentially bigger issue‚Äìhow we provide the energy needed to train models!\nI‚Äôm currently a Solutions Engineer at Bifrost.ai and am deeply embedded into the synthetic data space (pun intended)."
  },
  {
    "objectID": "about.html#previous-work",
    "href": "about.html#previous-work",
    "title": "About",
    "section": "Previous work",
    "text": "Previous work\nAs the first member of the Roboflow Field Engineering team, I was instrumental in growing the team to four plus members and establishing the foundation for the wider solutions engineering team.\nI drove customer growth and expansion, while making valuable engineering contributions to the wider customer base. In the last two quarters of 22‚Äô I generated ~400k in expansion based new revenue."
  },
  {
    "objectID": "about.html#outside-of-work",
    "href": "about.html#outside-of-work",
    "title": "About",
    "section": "Outside of work",
    "text": "Outside of work\nWhen not working, I focus on building a great family, training in the gym, and supporting my wife‚Äôs bakery.\nCheck out my work on LinkedIn and GitHub, or tweet at me"
  },
  {
    "objectID": "@archived/zzz_templates/welcome/index.html",
    "href": "@archived/zzz_templates/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "@archived/archived_posts/The Sovereign Tech Dad/index.html",
    "href": "@archived/archived_posts/The Sovereign Tech Dad/index.html",
    "title": "The Sovereign Tech Dad",
    "section": "",
    "text": "Learn why this blog exists, how to get value from it, and what it means to be a sovereign tech dad."
  },
  {
    "objectID": "@archived/archived_posts/The Sovereign Tech Dad/index.html#how-to-get-value",
    "href": "@archived/archived_posts/The Sovereign Tech Dad/index.html#how-to-get-value",
    "title": "The Sovereign Tech Dad",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "@archived/archived_posts/The Sovereign Tech Dad/index.html#why-sovereign-tech-dad",
    "href": "@archived/archived_posts/The Sovereign Tech Dad/index.html#why-sovereign-tech-dad",
    "title": "The Sovereign Tech Dad",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence enables you to take meaningful risks. If you have resources such as knowledge and good physical health, you can overcome greater challenges that lead to greater reward.\nGet enough of independence and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "@archived/archived_posts/The Sovereign Tech Dad/index.html#final-thoughts-and-a-call-to-action",
    "href": "@archived/archived_posts/The Sovereign Tech Dad/index.html#final-thoughts-and-a-call-to-action",
    "title": "The Sovereign Tech Dad",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "",
    "text": "Learn how a reinforcement learning (RF) algorithm uses policy and value functions to balance short vs long term rewards.\n\nNew to the blog? Start at Learn RL Fundamentals in Five Minutes Level 1."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#actions-have-results",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#actions-have-results",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#planning-for-future-actions",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#planning-for-future-actions",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL!"
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#conclusion",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#conclusion",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Conclusion",
    "text": "Conclusion\nEstablishing the proper policy and value functions provides context for reward.\nRewards can get complex, but finding the right balance of short and long term reward structures leads to the highest performing agents.\nTest yourself with the following questions: - ‚ÄúWhen should algorithms have a premium on short term rewards vs long term rewards‚Äù - ‚ÄúWhat enviroment factors should an engineer consider when creating dynamic value functions‚Äù\nTweet your answer to me at @ogjaylowe so we can have a chat about it! Would love to discuss."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#what-to-read-next",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_two/index.html#what-to-read-next",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "What to read next",
    "text": "What to read next\nReady to start coding?\nUp next read about creating a simple bandit."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html",
    "href": "@archived/archived_drafts/Bandit_1/index.html",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "",
    "text": "evaluation methods explained"
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#defining-a-bandit",
    "href": "@archived/archived_drafts/Bandit_1/index.html#defining-a-bandit",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Defining a bandit",
    "text": "Defining a bandit\nA bandit represents an environment, a set of rewards, and a set of actions.\nFor example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#why-bandits",
    "href": "@archived/archived_drafts/Bandit_1/index.html#why-bandits",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why bandits?",
    "text": "Why bandits?\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "href": "@archived/archived_drafts/Bandit_1/index.html#providing-bandit-actions-a-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Providing bandit actions a value",
    "text": "Providing bandit actions a value\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to exploit that to our advantage.\nLet‚Äôs create some perfect ground truth values for a three armed bandit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n\n\n\n\n\n\n\n\nUnfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\nWe can‚Äôt provide a static ground truth value for a bandit arm or else a greedy RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.\n\nA better action-value method\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.\nLet‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that as 2000 total action ‚Äútime steps‚Äù.\n\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "href": "@archived/archived_drafts/Bandit_1/index.html#greedy-and-nongreedy-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Greedy and nongreedy actions",
    "text": "Greedy and nongreedy actions\nAn agent will take a greedy action if it has been told to exploit its environment and choose the highest reward possible.\nIt will choose the nongreedy action during exploration in an attempt to better estimate the reward value an action will provide.\nOne should exploit once the optimal policy and value functions have been determined, as that will lead to maximum rewards.\nIdentifying the right ratio of exploitaton to exploration and the exploration decay rate plays a critical role in a succesfully converging an RL agent to optimal performance."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#meet-Œµ",
    "href": "@archived/archived_drafts/Bandit_1/index.html#meet-Œµ",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet Œµ",
    "text": "Meet Œµ\nThe ratio of when the agent chooses to exploit or explore commonly gets denoted with the greek notation of epsilon Œµ.\nIn general, exploration should be occur drastically less then exploit, as the agent should be focused on finding the maximum reward, so usually we set Œµ to equal to .1 or less.\n\nWhy exploration works\nIf you were to give an agent infinite resources and an Œµ value greater then zero, as it approaches the limit of infinity, it will exhaust all possible actions and thereby discover the ground truth values of a bandit.\nAgents do not have infinite resources but give them enough time and they will approach the limit close enough to accurately estimate the ground truth value to a certain acceptable degree."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#meet-the-agent",
    "href": "@archived/archived_drafts/Bandit_1/index.html#meet-the-agent",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Meet the agent",
    "text": "Meet the agent\nTODO - meet agent Rae?\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while getting jacked up on blocks falling would produce low reward (as they rile her up)."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "@archived/archived_drafts/Bandit_1/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "@archived/archived_drafts/Bandit_1/index.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#actions-have-results",
    "href": "@archived/archived_drafts/Bandit_1/index.html#actions-have-results",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "@archived/archived_drafts/Bandit_1/index.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#planning-for-future-actions",
    "href": "@archived/archived_drafts/Bandit_1/index.html#planning-for-future-actions",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "@archived/archived_drafts/Bandit_1/index.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#how-to-get-value",
    "href": "@archived/archived_drafts/Bandit_1/index.html#how-to-get-value",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "How to get value",
    "text": "How to get value\nThe five goals listed above map into five actionable categories:\n\ntake action on the fatherhood and developer actions immediately in your day-to-day living (each article will include examples on how to do this)\nrelate my summaries of technologies and the projects I‚Äôm building to your own working experience\nchallenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that\nexperiment with health changes to a rational degree (we have different bodies, be aware)\nlearn from, then emulate, the results of my content creation and marketing strategies\n\nFor dads:\nSuccess and life fulfillment come as a package. You need to have it all!\nIf for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?\nYou know best for your family.\nDon‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.\nFor everybody else:\nThe fatherhood components to my writing will not always be present and will be designated with a fatherhood tag for those who do not benefit from dad-related content.\nMaybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.\nThis blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!\nIf you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#why-sovereign-tech-dad",
    "href": "@archived/archived_drafts/Bandit_1/index.html#why-sovereign-tech-dad",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Why ‚ÄúSovereign‚Äù Tech Dad",
    "text": "Why ‚ÄúSovereign‚Äù Tech Dad\nWe live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.\nIndependence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.\nIdeally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.\nUse emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.\nI want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it."
  },
  {
    "objectID": "@archived/archived_drafts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "href": "@archived/archived_drafts/Bandit_1/index.html#final-thoughts-and-a-call-to-action",
    "title": "Creating and Solving Simple N-Arm Bandits with RL Agents",
    "section": "Final thoughts and a call to action",
    "text": "Final thoughts and a call to action\nPick the parts of the blog relevant to you.\nBuild valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.\nI want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!"
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html",
    "href": "@archived/archived_drafts/the_big_one.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "",
    "text": "TODO: when to be greedy, exploitation vs exploration explained\nTODO: action value methods?"
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html#how-an-rl-agent-knows-which-actions-to-take",
    "href": "@archived/archived_drafts/the_big_one.html#how-an-rl-agent-knows-which-actions-to-take",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "How an RL agent knows which actions to take",
    "text": "How an RL agent knows which actions to take\nAn environment can be used in many different ways depending on the goal an agent has at the time (imagine all the ways you can use your kitchent).\nThe policy determines which actions lead to the best outcome by mapping all actions possible, given the state of the environment, to a known reward value.\nFor example, an agent such as Rae operating in her bedroom will have seperate policies when playing vs going to sleep. Her playtime policy will put rewards on actions related to her toys while her sleep policy will place a reward on actions that calm her down."
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html#actions-have-results",
    "href": "@archived/archived_drafts/the_big_one.html#actions-have-results",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Actions have results",
    "text": "Actions have results\nAfter taking an action in its environment, the agent will measure how much progress it made towards its goal.\nThe amount of progess made takes the form of a singular number known as the reward signal. RL agents exist to find the path to maximum reward.\nRL agents will sometimes use the resulting reward to alter the policy mappings.\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime policy, she may value that action more next she plays."
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "href": "@archived/archived_drafts/the_big_one.html#rewards-have-both-an-immedate-and-long-term-payoff",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Rewards have both an immedate and long term payoff",
    "text": "Rewards have both an immedate and long term payoff\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term reward?\nThe value function bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\nRL agents will always attempt to find the optimal policy that leads to optimal rewards.\nComing back to Rae‚Äôs playtime policy, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive reward spike hits and she has the most fun possible.\nDimming the lights and reading stories during her sleeping policy might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep action occuring.\nAn effective value function represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of actions to reward."
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html#planning-for-future-actions",
    "href": "@archived/archived_drafts/the_big_one.html#planning-for-future-actions",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "Planning for future actions",
    "text": "Planning for future actions\nSometimes an RL agent will have access to an environment model that estimates the results of an action.\nA model may not always be available but can be particularly useful for games, or simple physics environments, in which clear causation exists.\nWhen I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a model that includes a temperature scale and the states of water for experimentation."
  },
  {
    "objectID": "@archived/archived_drafts/the_big_one.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "href": "@archived/archived_drafts/the_big_one.html#the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 2)",
    "section": "The difference between reinforcment learning, supervised learning, and unsupervised learning",
    "text": "The difference between reinforcment learning, supervised learning, and unsupervised learning\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\nRL can be combined with other ML techniques\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\nDeep reinforcment learning explained\nUse a deep neural network for your policy and you have deep RL.\nTODO - Break into three posts"
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "",
    "text": "Learn all the reinforcement learning (RL) fundamental concepts and terminology such as reward and value function in five minutes.\n\nTo help us better understand RL, I will be pairing technical writing with a simple analogy comparing an RL algorithm to that of a three year old.\nMy daughter was three at the time of this writing and it was fun explaining RL concepts to my non-technical wife as the kid jumps around and yells, so hopefully it helps you as well."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#meet-the-agent",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#meet-the-agent",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "Meet the agent",
    "text": "Meet the agent\nMy three year old agent Rae has a bedroom containing various features including toys, books, a bed, and much more.\nInside her bedroom environment, she can perform many actions such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\nDepending on if she wants to play or go to sleep‚Äìthe goal set for her in this bedroom will determine which actions lead high rewards.\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high reward while violently jumping on the bed would produce low reward (as goal == going to bed). If she wanted to play, which would probably be the case without intervention, then jumping on the bed would produce a high reward."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-can-you-solve-with-reinforcement-learning",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-can-you-solve-with-reinforcement-learning",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "What can you solve with reinforcement learning?",
    "text": "What can you solve with reinforcement learning?\nProblem statements with a well defined environment and bounded set of actions can typically be solved using RL methods.\nSome examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#conclusion",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#conclusion",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "Conclusion",
    "text": "Conclusion\nTest yourself with the following questions: - ‚ÄúCan everything truly be solved with by RL? If yes, list out some situations that would be impossible for an RL agent to work in.‚Äù - ‚ÄúDefine an environment, set of actions, an achevialbe goal that can be reached through those actions, and a relevant reward system to measure progress on that goal.‚Äù\nTweet your answer to me at @ogjaylowe so we can have a chat about it! Would love to discuss."
  },
  {
    "objectID": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-to-read-next",
    "href": "@archived/archived_posts/Learn_RL_Fudamentals_in_five_minutes_level_one/index.html#what-to-read-next",
    "title": "Learn RL Fundamentals in Five Minutes (Level 1)",
    "section": "What to read next",
    "text": "What to read next\nGot a hang for the fundamentals of RL and looking for more to read? Check out my post on additional complexities and components of an RL algorithm , or if you want to start coding, read about creating a simple bandit."
  },
  {
    "objectID": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html",
    "href": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html",
    "title": "Creating Simple N-Arm Bandits for RL Agents",
    "section": "",
    "text": "Learn how to create a simple N-armed bandit and train an RL agent to its optimal policy and value function."
  },
  {
    "objectID": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#defining-a-bandit",
    "href": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#defining-a-bandit",
    "title": "Creating Simple N-Arm Bandits for RL Agents",
    "section": "Defining a bandit",
    "text": "Defining a bandit\nA bandit represents an environment, a set of rewards, and a set of actions.\nFor example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1."
  },
  {
    "objectID": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-bandits",
    "href": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-bandits",
    "title": "Creating Simple N-Arm Bandits for RL Agents",
    "section": "Why bandits?",
    "text": "Why bandits?\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull."
  },
  {
    "objectID": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#providing-bandit-actions-a-value",
    "href": "@archived/archived_posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#providing-bandit-actions-a-value",
    "title": "Creating Simple N-Arm Bandits for RL Agents",
    "section": "Providing bandit actions a value",
    "text": "Providing bandit actions a value\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to exploit that to our advantage.\nLet‚Äôs create some perfect ground truth values for a three armed bandit.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n\n\n\n\n\n\n\n\nUnfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\nWe can‚Äôt provide a static ground truth value for a bandit arm or else a greedy RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.\n\nA better action-value method\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.\nLet‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that has 2000 total action ‚Äútime steps‚Äù.\n\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n\n\n\n\n\n\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function."
  },
  {
    "objectID": "@archived/zzz_templates/template/index.html",
    "href": "@archived/zzz_templates/template/index.html",
    "title": "Template Post",
    "section": "",
    "text": "Template plot containing code blocks, markdown features, and plots."
  },
  {
    "objectID": "@archived/zzz_templates/template/index.html#polar-axis",
    "href": "@archived/zzz_templates/template/index.html#polar-axis",
    "title": "Template Post",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure¬†1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 4 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis\n\n\n\n\n\n\n# importing module\nimport matplotlib.pyplot as plt\n \n# assigning x and y coordinates\nx = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\ny = []\n \nfor i in range(len(x)):\n    y.append(max(0, x[i]))\n \n# depicting the visualization\nplt.plot(x, y, color='green')\nplt.xlabel('X')\nplt.ylabel('Y')\n \n# square plot\nplt.axis('square')\n \n# displaying the title\nplt.title('ReLU Function')\n\nText(0.5, 1.0, 'ReLU Function')"
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-did-i-write-this",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#why-did-i-write-this",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Why did I write this?",
    "text": "Why did I write this?\nThe AI world has a lot of mania at the time of this writing, and as a ‚Äúcareer AI guy,‚Äù it is essential to understand the underlying forces that help drive (or kill üíÄ) the growth of the AI bubble.\nAlongside the mega-minds and hype men, the AI industry requires many ingredients such as computational power, infrastructure capabilities, and power generation.\nI don‚Äôt know much about how the energy space works as a whole, but I do know that AI requires a lot of it. How much specifically?\n\nAI servers could use 0.5% of the world‚Äôs electrical generation by 2027. For context, data centers currently use around 1% of global electrical generation‚Ä¶ this suggests an electricity consumption of approximately 3 Wh per LLM interaction.\n\nCell Press. (2023). Joule, Volume 7, Issue 7. Retrieved from https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3\nWhile writing this article, I made at least 100 LLM interactions, using about 300 Wh, so for context my AI usage was about the equivalent to a 10-watt LED light bulb running for 30 hours.\nMost importantly‚ÄîI can‚Äôt fix a problem that spans such a wide range of areas. Focusing on a specific issue in a specific industry could lead to a solvable stepping stone and better understanding the market forces as a whole!"
  },
  {
    "objectID": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "href": "posts/Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Understanding the energy sub-sectors and breaking them down into clusters",
    "text": "Understanding the energy sub-sectors and breaking them down into clusters\nI‚Äôm currently clustering sub-sectors into three primary categories:\n\nenergy generation ($4-5 trillion market cap)\n\nFossil Fuels: ~110,500 TWh (65%)\n\nCoal: ~44,200 TWh (40% of fossil fuels)\nNatural Gas: ~38,675 TWh (35% of fossil fuels)\nOil: ~27,625 TWh (25% of fossil fuels)\n\nRenewable Energy: ~46,000 TWh (27%)\n\nHydropower: ~18,400 TWh (40% of renewables)\nWind: ~9,200 TWh (20% of renewables)\nSolar: ~7,360 TWh (16% of renewables)\nBiomass: ~6,900 TWh (15% of renewables)\nGeothermal: ~3,680 TWh (8% of renewables)\nTidal/Wave: ~460 TWh (1% of renewables)\n\nNuclear Energy: ~13,500 TWh (8%)\n\nenergy storage ($2-3 trillion market cap)\n\nshort-term storage solutions ($350-400 billion market cap)\nlong-term storage solutions ($150-200 billion market cap)\n\nenergy transmission ($50-100 billion market cap)\n\nThe following have been grouped into tangential categories with significant influence on the energy space as a whole:\n\nenergy markets and trading\nenergy policy and regulation:\nenergy data and analytics:\nsustainability and environmental impact:\n\nThis follows a relatively intuitive flow: man generates energy ‚Üí man stores some of that energy ‚Üí man moves around and uses some of that energy.\nI‚Äôll outline the roadmap in this article and then delve into each of the sub-sectors in separate articles, as each represents multiple billion-dollar industries.\nFor example, the energy generation sub-sector includes fuel sources, methods of refinement, and utilization per fuel source (too much to detail here).\nThe following diagram represents the three primary categories defined above in a sunburst chart, with the size of each chunk representing its market cap relative to the energy space as a whole (roughly estimated to be seven trillion dollars for this example).\n\nimport plotly.graph_objects as go\n\n# Define the data\nlabels = [\n    \"Energy\",\n    \"Generation\", \"Storage\", \"Transmission\",\n    \"Fossil\", \"Renewable\", \"Nuclear\",\n    \"Short-term\", \"Long-term\",\n    \"Grid Infrastructure\", \"Efficiency\", \"Energy Conversion\",\n    \"Solar\", \"Wind\", \"Hydro\", \"geothermal\", \"biomass\", \"tidal\",\n    \"Oil\",\"Gas\",\"Coal\",\n    \"Lithium-ion batteries\", \"Supercapacitors\", \"Pumped hydro storage\", \"Compressed Air Energy Storage\",\n    \"Hydrogen storage\", \"Flow batteries\", \"Power-to-Gas\", \"Cryogenic energy storage\",\n]\n\nparents = [\n    \"\",\n    \"Energy\", \"Energy\", \"Energy\",\n    \"Generation\", \"Generation\", \"Generation\",\n    \"Storage\", \"Storage\",\n    \"Transmission\", \"Transmission\", \"Transmission\",\n    \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\",\n    \"Fossil\",\"Fossil\",\"Fossil\",\n    \"Short-term\", \"Short-term\", \"Short-term\", \"Short-term\",\n    \"Long-term\", \"Long-term\", \"Long-term\", \"Long-term\"\n]\n\n# Update values to represent market share (these are example values, adjust as needed)\nvalues = [\n    100,  # Energy (total)\n    69, 30, 1,  # Generation, Storage, Transmission\n    65, 27, 8,  # Fossil, Renewable, Nuclear\n    12.5, 37.5,  # Short-term, Long-term\n    7, 2.5, 2.5,  # Grid Infrastructure, Efficiency, Energy Conversion\n    16, 20, 40, 8, 15, 1,  # Solar, Wind, Hydro, Geothermal, Biomass, Tidal\n    25, 35, 45,  # Oil, Gas, Coal\n    38.5, 1.5, 12.5, 2.5,  # Lithium-ion, Supercapacitors, Pumped hydro, Compressed Air\n    22.5, 12.5, 12.5, 2.5,  # Hydrogen, Flow batteries, Power-to-Gas, Cryogenic\n]\n\n# Create the sunburst chart\nfig = go.Figure(go.Sunburst(\n    labels=labels,\n    parents=parents,\n    values=values,\n))\n\n# Update the layout\nfig.update_layout(\n    title=\"Energy Sector Sunburst Chart\",\n    width=800,\n    height=800,\n)\n\n# Show the chart\nfig.show()\n\n                                                \n\n\nThe final child nodes of the sunburst were calculated by estimating how much of the global 170,000 TWh (terawatt-hours) each energy source contributed.\nI‚Äôll be focusing on the tangential markets in a later post."
  },
  {
    "objectID": "posts/mapping_out_the_energy_space_part_1/index.html",
    "href": "posts/mapping_out_the_energy_space_part_1/index.html",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "",
    "text": "Understand the energy market‚Äôs sub-sectors and how the market as a whole connects together"
  },
  {
    "objectID": "posts/mapping_out_the_energy_space_part_1/index.html#why-did-i-write-this",
    "href": "posts/mapping_out_the_energy_space_part_1/index.html#why-did-i-write-this",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Why did I write this?",
    "text": "Why did I write this?\nThe AI world has a lot of mania at the time of this writing, and as a ‚Äúcareer AI guy,‚Äù it is essential to understand the underlying forces that help drive (or kill üíÄ) the growth of the AI bubble.\nAlongside the mega-minds and hype men, the AI industry requires many ingredients such as computational power, infrastructure capabilities, and power generation.\nI don‚Äôt know much about how the energy space works as a whole, but I do know that AI requires a lot of it. How much specifically?\n\nAI servers could use 0.5% of the world‚Äôs electrical generation by 2027. For context, data centers currently use around 1% of global electrical generation‚Ä¶ this suggests an electricity consumption of approximately 3 Wh per LLM interaction.\n\n\nCell Press. (2023). Joule, Volume 7, Issue 7. Retrieved from https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3 &gt;\n\nWhile writing this article, I made at least 100 LLM interactions, using about 300 Wh, so for context my AI usage was about the equivalent to a 10-watt LED light bulb running for 30 hours.\nMost importantly‚ÄîI can‚Äôt fix a problem that spans such a wide range of areas. Focusing on a specific issue in a specific industry could lead to a solvable stepping stone and better understanding the market forces as a whole!"
  },
  {
    "objectID": "posts/mapping_out_the_energy_space_part_1/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "href": "posts/mapping_out_the_energy_space_part_1/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "Understanding the energy sub-sectors and breaking them down into clusters",
    "text": "Understanding the energy sub-sectors and breaking them down into clusters\nI‚Äôm currently clustering sub-sectors into three primary categories:\n\nenergy generation ($4-5 trillion market cap)\n\nFossil Fuels: ~110,500 TWh (65%)\n\nCoal: ~44,200 TWh (40% of fossil fuels)\nNatural Gas: ~38,675 TWh (35% of fossil fuels)\nOil: ~27,625 TWh (25% of fossil fuels)\n\nRenewable Energy: ~46,000 TWh (27%)\n\nHydropower: ~18,400 TWh (40% of renewables)\nWind: ~9,200 TWh (20% of renewables)\nSolar: ~7,360 TWh (16% of renewables)\nBiomass: ~6,900 TWh (15% of renewables)\nGeothermal: ~3,680 TWh (8% of renewables)\nTidal/Wave: ~460 TWh (1% of renewables)\n\nNuclear Energy: ~13,500 TWh (8%)\n\nenergy storage ($2-3 trillion market cap)\n\nshort-term storage solutions ($350-400 billion market cap)\nlong-term storage solutions ($150-200 billion market cap)\n\nenergy transmission ($50-100 billion market cap)\n\nThe following have been grouped into tangential categories with significant influence on the energy space as a whole:\n\nenergy markets and trading\nenergy policy and regulation:\nenergy data and analytics:\nsustainability and environmental impact:\n\nThis follows a relatively intuitive flow: man generates energy ‚Üí man stores some of that energy ‚Üí man moves around and uses some of that energy.\nI‚Äôll outline the roadmap in this article and then delve into each of the sub-sectors in separate articles, as each represents multiple billion-dollar industries.\nFor example, the energy generation sub-sector includes fuel sources, methods of refinement, and utilization per fuel source (too much to detail here).\nThe following diagram represents the three primary categories defined above in a sunburst chart, with the size of each chunk representing its market cap relative to the energy space as a whole (roughly estimated to be seven trillion dollars for this example).\n\nimport plotly.graph_objects as go\n\n# Define the data\nlabels = [\n    \"Energy\",\n    \"Generation\", \"Storage\", \"Transmission\",\n    \"Fossil\", \"Renewable\", \"Nuclear\",\n    \"Short-term\", \"Long-term\",\n    \"Grid Infrastructure\", \"Efficiency\", \"Energy Conversion\",\n    \"Solar\", \"Wind\", \"Hydro\", \"geothermal\", \"biomass\", \"tidal\",\n    \"Oil\",\"Gas\",\"Coal\",\n    \"Lithium-ion batteries\", \"Supercapacitors\", \"Pumped hydro storage\", \"Compressed Air Energy Storage\",\n    \"Hydrogen storage\", \"Flow batteries\", \"Power-to-Gas\", \"Cryogenic energy storage\",\n]\n\nparents = [\n    \"\",\n    \"Energy\", \"Energy\", \"Energy\",\n    \"Generation\", \"Generation\", \"Generation\",\n    \"Storage\", \"Storage\",\n    \"Transmission\", \"Transmission\", \"Transmission\",\n    \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\",\n    \"Fossil\",\"Fossil\",\"Fossil\",\n    \"Short-term\", \"Short-term\", \"Short-term\", \"Short-term\",\n    \"Long-term\", \"Long-term\", \"Long-term\", \"Long-term\"\n]\n\n# Update values to represent market share (these are example values, adjust as needed)\nvalues = [\n    100,  # Energy (total)\n    69, 30, 1,  # Generation, Storage, Transmission\n    65, 27, 8,  # Fossil, Renewable, Nuclear\n    12.5, 37.5,  # Short-term, Long-term\n    7, 2.5, 2.5,  # Grid Infrastructure, Efficiency, Energy Conversion\n    16, 20, 40, 8, 15, 1,  # Solar, Wind, Hydro, Geothermal, Biomass, Tidal\n    25, 35, 45,  # Oil, Gas, Coal\n    38.5, 1.5, 12.5, 2.5,  # Lithium-ion, Supercapacitors, Pumped hydro, Compressed Air\n    22.5, 12.5, 12.5, 2.5,  # Hydrogen, Flow batteries, Power-to-Gas, Cryogenic\n]\n\n# Create the sunburst chart\nfig = go.Figure(go.Sunburst(\n    labels=labels,\n    parents=parents,\n    values=values,\n))\n\n# Update the layout\nfig.update_layout(\n    title=\"Energy Sector Sunburst Chart\",\n    width=800,\n    height=800,\n)\n\n# Show the chart\nfig.show()\n\n                                                \n\n\nThe final child nodes of the sunburst were calculated by estimating how much of the global 170,000 TWh (terawatt-hours) each energy source contributed.\nI‚Äôll be focusing on the tangential markets in a later post."
  },
  {
    "objectID": "posts/mapping_out_the_energy_space_part_1/index.html#what-to-read-next",
    "href": "posts/mapping_out_the_energy_space_part_1/index.html#what-to-read-next",
    "title": "Understanding Energy Sub-Sectors: Part 1",
    "section": "What to read next",
    "text": "What to read next\nPost link to next article here‚Ä¶"
  },
  {
    "objectID": "posts/p3_energy_transmission/index.html",
    "href": "posts/p3_energy_transmission/index.html",
    "title": "How the Heck Do We Transmit Energy: [PART 3]",
    "section": "",
    "text": "Understand how energy transmission works from a fundamental perspective\n\n\nAt a high level, electrical energy in the form of electromagnetic waves zips through the vacuum of space at the speed of light. But here on Earth, we have to shuffle energy along conductive materials such as copper wire. This physical route slows things down by a hefty amount!\nTLDR we gotta push it real good!\nTo effectively discuss and explain energy transfer processes, it‚Äôs crucial to grasp these key concepts:\n\nvoltage (V = I x R): similar to pressure in a water pipe, voltage can represent how much ‚Äúpush‚Äù an electrical charge will have with higher amounts of voltage corresponding to faster traveling times.\n\ncurrent (I): the current rate of travel energy will move through a conductive material at, with higher currents representing a higher rate of ‚Äúelectrical flow‚Äù\nresistance (R): the forces acting against the ‚Äúelectrical flow‚Äù, similar to friction\nNote: typically as voltage increases, current increases as well because in most contexts the physical conduit that energy travel along will remain the same unless the material gets replaced (or worn down by long periods of time)\n\nwatt (aka power, P = V x I): represents the rate of ‚Äúpushing‚Äù, or work being done, will occur‚Äîwith higher levels of power having more ‚Äúpush‚Äù, a higher rate of ‚Äúpush‚Äù, or both.\n\nNote: the relationship of voltage and power can also be represented as:\n\nP = (I √ó R) √ó I\nP = I^2 √ó R\nI = V / R\n\ncurrent is directly proportionate to resistance (double one and the other halves)\ndoubling the current quadruples the power (therefore power will always be sensitive to current)\n\n\nWhile P = V x I and P = I^2 √ó R operate in an algebraically equivalently manner:\n\nP = V x I represents the total power transmitted through a system\nP = I^2 √ó R represents the power lost as heat due to resistance, aka Joule heating\n\n\njoule (aka energy, E = integral of P over time): the total amount of energy that has flowed through the system, with higher joules representing more energy"
  },
  {
    "objectID": "posts/p3_energy_transmission/index.html#step-to-the-left-now-step-to-the-right",
    "href": "posts/p3_energy_transmission/index.html#step-to-the-left-now-step-to-the-right",
    "title": "How the Heck Do We Transmit Energy: [PART 3]",
    "section": "Step to the left, now step to the right",
    "text": "Step to the left, now step to the right\nTo provide context for what qualifies as ‚Äúhigh‚Äù vs.¬†‚Äúlow‚Äù voltage, the average American household uses 110-volt power outlets for most appliances.\nPower generation facilities typically produce electricity between 10-25 kV (averging 127 times greater than what powers your toaster).\nNewly generated energy often travels 50-300 miles after production, necessitating stepped-up voltage levels for efficient transmission.\n\nFeelings of powerlessness\nTransmission lines provide a conduit for energy to travel through, but also resistance. The majority of power loss will be caused by this resistance.\nHere‚Äôs the fancy workaround trick engineers use to maximize energy transfer:\n\nSince power = voltage * current (I), increasing the voltage will result in a lower current if power remains constant\nPower loss responds to current quadratically in power loss = current^2 √ó resistance , so a decrease in current leads to major efficiency improvement in power loss\n\nexample with 1,000,000 W of power, 10 Œ© of resistance, and 10,000 V :\n\nusing I = P / V to calculate current, 1,000,000 W / 10,000 V = 100 A\nusing P = I^2 * R to calculate power loss, (100 A)^2 * 10 Œ© = 100,000 W of power lost to resistance\n\nexample with 1,000,000 W of power, 10 Œ© of resistance, and 100,000 V :\n\nusing I = P / V to calculate current, 1,000,000 W / 100,000 V = 10 A\nusing P = I^2 * R to calculate power loss, (10 A)^2 * 10 Œ© = 1,000 W of power lost to resistance\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Provided definitions\ndef calculate_current(power, voltage):\n    return power / voltage\n\ndef calculate_power_lost(current, resistance):\n    return (current**2) * resistance\n\n# Initialization\npower_in_watts = 1000000\nresistance_in_ohms = 10\nlow_voltage = 10000\nhigh_voltage = low_voltage * 10\n\n# Example 1 - power lost when using low_voltage\nlow_voltage_current = calculate_current(power_in_watts, low_voltage)\npower_lost_to_low_voltage = calculate_power_lost(low_voltage_current, resistance_in_ohms)\n\n# Example 2 - power lost when using high_voltage\nhigh_voltage_current = calculate_current(power_in_watts, high_voltage)\npower_lost_to_high_voltage = calculate_power_lost(high_voltage_current, resistance_in_ohms)\n\n# Data for plotting\nvoltages = [low_voltage, high_voltage]\ncurrents = [low_voltage_current, high_voltage_current]\npower_losses = [power_lost_to_low_voltage, power_lost_to_high_voltage]\n\n# Plotting the results\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()\nax1.plot(voltages, currents, 'g-', marker='o', label=\"Current (A)\")\nax2.plot(voltages, power_losses, 'b-', marker='o', label=\"Power Loss (W)\")\n\nax1.set_xlabel('Voltage (V)')\nax1.set_ylabel('Current (A)', color='g')\nax2.set_ylabel('Power Loss (W)', color='b')\n\n# Adjusting the scale of the power loss axis to show quadratic relationship\nax2.set_yscale('log')\n\n# Adding an index to the side to show exact values\nfor i, (v, c, p) in enumerate(zip(voltages, currents, power_losses)):\n    ax1.annotate(f'V: {v}\\nI: {c:.2f} A\\nP_loss: {p:.2e} W',\n                 xy=(v, c), xytext=(5, -10 - 30 * i),\n                 textcoords='offset points', ha='center', va='center',\n                 fontsize=9, color='black', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgray\"))\n\n# Centering the figure title\nplt.title('Quadratic Relationship between Voltage, Current, and Power Loss', pad=20)\n\nplt.show()\n\n\n\n\n\n\n\n\nTLDR a 10x increase in voltage results in a 100x less power lost because less current means less power lost to heat!"
  },
  {
    "objectID": "posts/p3_energy_transmission/index.html#now-step-it-up",
    "href": "posts/p3_energy_transmission/index.html#now-step-it-up",
    "title": "How the Heck Do We Transmit Energy: [PART 3]",
    "section": "Now step it up!",
    "text": "Now step it up!\n\n\n\nFig 1 - diagram of energy transmission\n\n\n\nSustainable Sanitation and Water Management (SSWM). (n.d.). Hydropower (large-scale). SSWM. Retrieved August 21, 2024, from https://sswm.info/water-nutrient-cycle/water-use/hardwares/water-energy/hydropower-(large-scale)\n\nLet‚Äôs follow the flow of energy as it moves from generator to home appliance:\n\nGenerator: Produces energy, usually via induction (such as at a hydroelectric dam), converting mechanical energy into electrical energy at ~10-25 kV\nStep-up transformer: Increases voltage to ~250-500 kV, depending on the distance to the next step\nCircuit breaker: Protects the entire electrical system by ensuring interruptions in flow from one section don‚Äôt cause faults or overloads in another\nHigh voltage lines: Provide the conduit for electricity to travel over long distances\nStep-down transformer (local substation): Decreases voltage to ~5-50 kV range for transfer in local areas\nStep-down transformer (pole transformer): Further decreases voltage to the standard 120 or 240 volts used in residential outlets"
  },
  {
    "objectID": "posts/p3_energy_transmission/index.html#next-up-to-read",
    "href": "posts/p3_energy_transmission/index.html#next-up-to-read",
    "title": "How the Heck Do We Transmit Energy: [PART 3]",
    "section": "Next up to read",
    "text": "Next up to read\nNow that the fundamental mechanics of energy transmission has been covered, lets learn a bit more about how new energy sources get added to the grid‚Äîand whom wields the power!"
  },
  {
    "objectID": "posts/p1_mapping_out_the_energy_space/index.html",
    "href": "posts/p1_mapping_out_the_energy_space/index.html",
    "title": "Understanding Energy Sub-Sectors: [PART 1]",
    "section": "",
    "text": "Understand the energy market‚Äôs sub-sectors and how the market as a whole connects together\n\n\nA quick foreword‚ÄîI‚Äôm an AI guy and I use it for lots of things.\nWhile I don‚Äôt like to use it for writing, as it tends to create voiceless and untrustworthy text, it does one thing REALLY well‚Ä¶.\n\n\n\nFig 1 - an okay meme\n\n\nClaude helped me to brainstorm the list (which I heavily curated and modified) that I‚Äôm using as my roadmap for understanding the energy market as a whole."
  },
  {
    "objectID": "posts/p1_mapping_out_the_energy_space/index.html#why-did-i-write-this",
    "href": "posts/p1_mapping_out_the_energy_space/index.html#why-did-i-write-this",
    "title": "Understanding Energy Sub-Sectors: [PART 1]",
    "section": "Why did I write this?",
    "text": "Why did I write this?\nThe AI world has a lot of mania at the time of this writing, and as a ‚Äúcareer AI guy,‚Äù it is essential to understand the underlying forces that help drive (or kill üíÄ) the growth of the AI bubble.\nAlongside the mega-minds and hype men, the AI industry requires many ingredients such as computational power, infrastructure capabilities, and power generation.\nI don‚Äôt know much about how the energy space works as a whole, but I do know that AI requires a lot of it. How much specifically?\n\nAI servers could use 0.5% of the world‚Äôs electrical generation by 2027. For context, data centers currently use around 1% of global electrical generation‚Ä¶ this suggests an electricity consumption of approximately 3 Wh per LLM interaction.\n\n\nCell Press. (2023). Joule, Volume 7, Issue 7. Retrieved from https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3 &gt;\n\nWhile writing this article, I made at least 100 LLM interactions, using about 300 Wh, so for context my AI usage was about the equivalent to a 10-watt LED light bulb running for 30 hours.\nMost importantly‚ÄîI can‚Äôt fix a problem that spans such a wide range of areas. Focusing on a specific issue in a specific industry could lead to a solvable stepping stone and better understanding the market forces as a whole!"
  },
  {
    "objectID": "posts/p1_mapping_out_the_energy_space/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "href": "posts/p1_mapping_out_the_energy_space/index.html#understanding-the-energy-sub-sectors-and-breaking-them-down-into-clusters",
    "title": "Understanding Energy Sub-Sectors: [PART 1]",
    "section": "Understanding the energy sub-sectors and breaking them down into clusters",
    "text": "Understanding the energy sub-sectors and breaking them down into clusters\nI‚Äôm currently clustering sub-sectors into three primary categories:\n\nenergy generation ($4-5 trillion market cap)\n\nFossil Fuels: ~110,500 TWh (65%)\n\nCoal: ~44,200 TWh (40% of fossil fuels)\nNatural Gas: ~38,675 TWh (35% of fossil fuels)\nOil: ~27,625 TWh (25% of fossil fuels)\n\nRenewable Energy: ~46,000 TWh (27%)\n\nHydropower: ~18,400 TWh (40% of renewables)\nWind: ~9,200 TWh (20% of renewables)\nSolar: ~7,360 TWh (16% of renewables)\nBiomass: ~6,900 TWh (15% of renewables)\nGeothermal: ~3,680 TWh (8% of renewables)\nTidal/Wave: ~460 TWh (1% of renewables)\n\nNuclear Energy: ~13,500 TWh (8%)\n\nenergy storage ($2-3 trillion market cap)\n\nshort-term storage solutions ($350-400 billion market cap)\nlong-term storage solutions ($150-200 billion market cap)\n\nenergy transmission ($50-100 billion market cap)\n\nThe following have been grouped into tangential categories with significant influence on the energy space as a whole:\n\nenergy markets and trading\nenergy policy and regulation:\nenergy data and analytics:\nsustainability and environmental impact:\n\nThis follows a relatively intuitive flow: man generates energy ‚Üí man stores some of that energy ‚Üí man moves around and uses some of that energy.\nI‚Äôll outline the roadmap in this article and then delve into each of the sub-sectors in separate articles, as each represents multiple billion-dollar industries.\nFor example, the energy generation sub-sector includes fuel sources, methods of refinement, and utilization per fuel source (too much to detail here).\nThe following diagram represents the three primary categories defined above in a sunburst chart, with the size of each chunk representing its market cap relative to the energy space as a whole (roughly estimated to be seven trillion dollars for this example).\n\nimport plotly.graph_objects as go\n\n# Define the data\nlabels = [\n    \"Energy\",\n    \"Generation\", \"Storage\", \"Transmission\",\n    \"Fossil\", \"Renewable\", \"Nuclear\",\n    \"Short-term\", \"Long-term\",\n    \"Grid Infrastructure\", \"Efficiency\", \"Energy Conversion\",\n    \"Solar\", \"Wind\", \"Hydro\", \"geothermal\", \"biomass\", \"tidal\",\n    \"Oil\",\"Gas\",\"Coal\",\n    \"Lithium-ion batteries\", \"Supercapacitors\", \"Pumped hydro storage\", \"Compressed Air Energy Storage\",\n    \"Hydrogen storage\", \"Flow batteries\", \"Power-to-Gas\", \"Cryogenic energy storage\",\n]\n\nparents = [\n    \"\",\n    \"Energy\", \"Energy\", \"Energy\",\n    \"Generation\", \"Generation\", \"Generation\",\n    \"Storage\", \"Storage\",\n    \"Transmission\", \"Transmission\", \"Transmission\",\n    \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\", \"Renewable\",\n    \"Fossil\",\"Fossil\",\"Fossil\",\n    \"Short-term\", \"Short-term\", \"Short-term\", \"Short-term\",\n    \"Long-term\", \"Long-term\", \"Long-term\", \"Long-term\"\n]\n\n# Update values to represent market share (these are example values, adjust as needed)\nvalues = [\n    100,  # Energy (total)\n    69, 30, 1,  # Generation, Storage, Transmission\n    65, 27, 8,  # Fossil, Renewable, Nuclear\n    12.5, 37.5,  # Short-term, Long-term\n    7, 2.5, 2.5,  # Grid Infrastructure, Efficiency, Energy Conversion\n    16, 20, 40, 8, 15, 1,  # Solar, Wind, Hydro, Geothermal, Biomass, Tidal\n    25, 35, 45,  # Oil, Gas, Coal\n    38.5, 1.5, 12.5, 2.5,  # Lithium-ion, Supercapacitors, Pumped hydro, Compressed Air\n    22.5, 12.5, 12.5, 2.5,  # Hydrogen, Flow batteries, Power-to-Gas, Cryogenic\n]\n\n# Create the sunburst chart\nfig = go.Figure(go.Sunburst(\n    labels=labels,\n    parents=parents,\n    values=values,\n))\n\n# Update the layout\nfig.update_layout(\n    title=\"Energy Sector Sunburst Chart\",\n    width=800,\n    height=800,\n)\n\n# Show the chart\nfig.show()\n\n                                                \n\n\nThe final child nodes of the sunburst were calculated by estimating how much of the global 170,000 TWh (terawatt-hours) each energy source contributed.\nI‚Äôll be focusing on the tangential markets in a later post."
  },
  {
    "objectID": "posts/p1_mapping_out_the_energy_space/index.html#what-to-read-next",
    "href": "posts/p1_mapping_out_the_energy_space/index.html#what-to-read-next",
    "title": "Understanding Energy Sub-Sectors: [PART 1]",
    "section": "What to read next",
    "text": "What to read next\nNext up we dig into hydroelectric power!"
  },
  {
    "objectID": "posts/p2_hydro_electric_power_part/index.html",
    "href": "posts/p2_hydro_electric_power_part/index.html",
    "title": "DAMS DAMS DAMS: [PART 2]",
    "section": "",
    "text": "How hydroelectric dams produce energy and the way Idaho uses it to power the majority of its cities\n\n\nNow that energy sectors have been broken down, let‚Äôs start exploring energy sources.\nSince the first human saw the first river, we‚Äôve been doing everything we can to ruin them. Some might say it was a match made in heaven. In fact, 3000 BCE was the beginning of serious river fuckery‚Äîbut honestly we‚Äôve probably been doing it for longer than that!\n\nHydroelectric powerplants are the most efficient means of producing electric energy. The efficiency of today‚Äôs hydroelectric plant is about 90 percent\n\n\nU.S. Bureau of Reclamation. (n.d.). Hydropower program. U.S. Department of the Interior. Retrieved from https://www.usbr.gov/power/edu/pamphlet.pdf\n\nThat quote was taken before zoomers existed so hopefully we have achieved something higher than 90%.\nIn terms of ‚Äúsexy and lucrative emerging business ventures‚Äù‚Äîdams score pretty low. Which makes it ripe for innovation!"
  },
  {
    "objectID": "posts/p2_hydro_electric_power_part/index.html#differentiating-types-of-hydroelectric-power-generation",
    "href": "posts/p2_hydro_electric_power_part/index.html#differentiating-types-of-hydroelectric-power-generation",
    "title": "DAMS DAMS DAMS: [PART 2]",
    "section": "Differentiating types of hydroelectric power generation",
    "text": "Differentiating types of hydroelectric power generation\nFossil fuels, renewable, and fusion energy sources all play a role in the nations energy portfolio, but in Idaho‚Äîaround 43% of of the energy generated comes from hydroelectric facilities (5th highest in the nation)!\nFive of the ten facilities listed in Idaho‚Äôs renewable energy breakdown operate as hydroelectric dams.\nWhat might the other forms of hydroelectric generation be?\nLet‚Äôs break down the most common forms of hydroelectric power generation:\n\nHydroelectric dams: A large wall built across the width of a river, called a dam, causes a significant reservoir of water to build up. This water can be released at a controlled rate through turbines built into the dam‚Äôs structure.\n\nPumped storage facilities can use excess energy to pump water from a lower point to a higher point. This water can then be released through turbines during periods of high energy demand.\n\nRun-of-the-river hydroelectric plants: A turbine uses the natural flow of the river without creating a reservoir.\nSmall-scale hydro projects: Plants built into existing infrastructure, such as a water supply pipeline, generate electricity without a reservoir."
  },
  {
    "objectID": "posts/p2_hydro_electric_power_part/index.html#hydroelectric-dam-energy-generation-efficiency",
    "href": "posts/p2_hydro_electric_power_part/index.html#hydroelectric-dam-energy-generation-efficiency",
    "title": "DAMS DAMS DAMS: [PART 2]",
    "section": "Hydroelectric dam energy generation efficiency",
    "text": "Hydroelectric dam energy generation efficiency\nEnergy generation facilities maximize the output of energy while minimizing energy loss throughout the system.\nSimilar to a Costco hotdog going through the human digestive track‚Äîundesirable forces tend to ruin good times. For a dam, forces such as turbine friction and water turbulence can cause energy that could have been captured elsewhere to be lost counterproductively.\nTake a look at this diagram provided by the USGS for a better understanding of the process as a whole.\n\n\n\nFig 1 - hydro power plant diagram from USGS\n\n\n\nPotential Energy Storage: Water held in the reservoir stores potential energy due to its elevation\nPenstock: Water flows down a large pipe called the penstock, converting potential energy to kinetic energy\nTurbine: Fast-moving water spins a turbine, creating mechanical energy from the rotating turbine shaft\nGenerator: The turbine shaft spins, causing a large electromagnet inside the generator to rotate past stationary coils of wire‚Äîinducing an electric current through electromagnetic induction (converting mechanical energy to electrical energy)\nDam Transformer: Lower voltage electricity produced by the generator passes through the high voltage transformer for more efficient long-distance transmission\n\nThis step doesn‚Äôt create new energy but changes the characteristics of the electrical energy (higher voltage, lower current)\n\nTransmission Lines: High voltage electricity travels across transmission lines to substations\nSubstation Transformers: High voltage electricity passes through the low voltage transformer for distribution to local areas\nDistribution Lines: Lower voltage electricity flows directly to homes and businesses\nLocal Transformers: Transformers on power poles or in ground-level boxes further reduce the voltage for use in buildings\nApplication Usage: Homes and businesses utilize the energy in various forms (light, heat, mechanical energy in motors, etc.)\n\nTLDR spin to win baby!\n\nPublic vs private energy generation\nSimilar to any other market sector, generation of energy can be a venture owned by private, public, or government corporations. In Idaho it breaks down like this:\n\nPrivate facilities\n\nHells Canyon Complex (a collection of three facilities) = largest privately-owned conventional hydroelectric generating facility in the USA\n\nBrownlee Dam\nOxbow Dam\nHells Canyon Dam\n\nC.J. Strike Dam\nLower Salmon Falls Dam\nCabinet Gorge Dam (only one not owned by Idaho Power, but rather owned by Avista)\nBliss Dam\nUpper Salmon Falls Dam\nSwan Falls Dam\n\nPublic facilities\n\nDworshak Dam\n\nState / federal facilities\n\nPriest Lake Dam\nAmerican Falls Dam\n\n\n\n\nSomewhat unrelated but interesting facts on powers biggest ‚Äúpower users‚Äù in Idaho\n\n30% of power goes to industrial usage (it is the gem state after all)\n30% of power goes to transportation (Idaho has no petrolium production facilities and has rural transportation capabilities)\n24% goes to residential usage\n16% goes to commercial usage\n\nIdaho consumes 4x more power than it generates (the rest comes from natural gas imports and the Bonneville Power Administration (BPA) which reroutes electric energy from federal dams out-of-state). Surprisingly, Idaho still has the nation‚Äôs third-lowest average residential electricity price.\n\n\n\nFig 2 - not actually Figure 1.10\n\n\n\nIdaho Office of Energy and Mineral Resources. (2021). Idaho energy landscape 2021. https://oemr.idaho.gov/wp-content/uploads/Idaho-Energy-Landscape-2021.pdf\n\nAs a result of this gap between consumption and production, the Pacific Northwest‚Äôs transmission capabilities becomes more congested each year as demands increase across the entire region. Solar and wind energy have the most new projects coming up in the next decade and funding has been funneled towards more.\n\n\n\nFig 3 - in-state energy production vs out-of-state energy import\n\n\n\nU.S. Energy Information Administration. (n.d.). Idaho - State electricity profile. U.S. Energy Information Administration. Retrieved August 21, 2024, from https://www.eia.gov/electricity/state/idaho/index.php\n\nWhile annual energy consumption rates in Idaho increase at an annual rate of about 2.5%, energy imports have been decreasing\nMore reports on Idaho energy can be found here."
  },
  {
    "objectID": "posts/p2_hydro_electric_power_part/index.html#what-to-read-next",
    "href": "posts/p2_hydro_electric_power_part/index.html#what-to-read-next",
    "title": "DAMS DAMS DAMS: [PART 2]",
    "section": "What to read next",
    "text": "What to read next\nAn exploration on transformers and how energy works from a fundamental level!"
  },
  {
    "objectID": "posts/p4_by_the_power_of_regulation/index.html",
    "href": "posts/p4_by_the_power_of_regulation/index.html",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "",
    "text": "Learn how the powergrid gets congested and the regulation bodies responsible for all things powergrid related\n\n\nWe know where energy comes from, and we know how to can be transmitted‚Äîbut how does new energy get added to the grid and whom controls it?"
  },
  {
    "objectID": "posts/p4_by_the_power_of_regulation/index.html#the-grid",
    "href": "posts/p4_by_the_power_of_regulation/index.html#the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "The grid",
    "text": "The grid\nSimilar to the internet, the nations power grid operates as a vast network of transmission cables and routing substations to deliver electricity from producer to consumer. The three primary components include:\n\ngeneration\ntransmission (high-voltage lines between producers)\ndistribution (low-voltage lines to consumers)\n\n\nCongestion\nPhysical limitations in the grid mean the amount of power available in an area will always have a cap on it.\nOften times during certain times of a season or during major events, demands for electricity will exceed the capacity of the grid for a short time.\nWorst case scenario would be excess power generation, in a low demand area, with insufficient transmission capacity to move it to areas of high demand.\nOutages in production, or temporary maintenance of a line, will increase electrical load on remaining lines.\nBottlenecks will form in areas that contain multiple lines converging‚Äîleading to higher levels of energy congestion."
  },
  {
    "objectID": "posts/p4_by_the_power_of_regulation/index.html#connecting-to-the-grid",
    "href": "posts/p4_by_the_power_of_regulation/index.html#connecting-to-the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "Connecting to the grid",
    "text": "Connecting to the grid\nWhen a new power generation facility, such as a wind farm or new hydroelectric dam, finishes production‚Äîtransmission lines will be connected from the new facilities step-up transformer (matching existing voltage and frequency) to the nearest substation.\nOnce hooked up to the grid, the new production facility will gradually increase power output until complete synchronization!"
  },
  {
    "objectID": "posts/p4_by_the_power_of_regulation/index.html#regulating-the-grid",
    "href": "posts/p4_by_the_power_of_regulation/index.html#regulating-the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "Regulating the grid",
    "text": "Regulating the grid\nPower is power, so-to-speak, and everybody wants it! As a result, several large governing bodies have to be involved with any major power change:\n\nFederal Energy Regulatory Commission (FERC): an independent agency which regulates the interstate transmission of electricity and oversees wholesale electricity markets\n\nex: the FERC would approve a proposal to build new interstate transmission lines between Arizona and California\n\nNorth American Electric Reliability Corporation (NERC): A non-profit organization that develops and enforces reliability standards for the bulk power system in North America\n\nex: NERC developed and enforced a new cybersecurity standard for power grid operators after a series of attempted cyberattacks on utility companies.\n\nRegional Transmission Organizations (RTOs) and Independent System Operators (ISOs): non-profit organizations that coordinate, control, and monitor the electric grid across multiple states and helps manage the wholesale electricity markets to ensure reliable operation of the grid\n\nex: during heatwaves in the Midwest, the Midcontinent Independent System Operator (MISO) coordinates with power plants across several states to increase electricity generation and manage the grid to prevent blackouts\n\nState Public Utility Commissions (PUCs): state-level agencies that regulate the retail electricity markets and oversee the operations of utilities within their state\n\nex: The California Public Utilities Commission (CPUC) approved a rate increase for a major utility company to fund wildfire prevention measures\n\nLocal utilities: companies responsible for the final distribution of electricity to end-users and maintaining the local distribution infrastructure\n\nex: A municipal utility located within a small town in Oregon implemented a smart meter program for all its customers, allowing for more accurate billing\n\n\nThese governing bodies have created various frameworks to understand and control the market forces that dictate when new energy needs to be produced, how it will transfer there, and who will get to use it.\nRTOs and ISOs have algorithms and market mechanisms to optimize how much energy powers plant should be producing and the capacity of operation required to meet market demands.\n\nHow this maps to the great state of Idaho\nDue to its heavy reliance on hydroelectric power, Idaho‚Äôs energy landscape has a relatively simple regulatory framework compared to states with multiple overlapping RTOs or ISOs.\nHere‚Äôs how it breaks down:\n\nFERC: Issues licensing and oversees projects the Snake River as it is considered a major interstate water resource and therefore everything utilizing it must be built within compliance of federal regulation.\nNERC: Idaho falls under the Western Electricity Coordinating Council (WECC), which is the regional entity of NERC responsible for coordinating and promoting bulk electric system reliability in the Western Interconnection.\nRTO/ISO: Idaho does not belong to any Regional Transmission Organization (RTO) or Independent System Operator (ISO). Most of the state‚Äôs transmission system is operated by individual utilities\nPUC (Public Utilities Commission): Idaho has one state-level regulatory body‚Äî theIdaho Public Utilities Commission (IPUC)\nMajor Utilities in Idaho: While not regulatory bodies themselves, these are the main companies regulated by the IPUC:\n\nIdaho Power Company\nAvista Utilities\nRocky Mountain Power (a division of PacifiCorp)\nFall River Rural Electric Cooperative\nKootenai Electric Cooperative"
  },
  {
    "objectID": "posts/p4_by_the_power_of_regulation/index.html#next-up-to-read",
    "href": "posts/p4_by_the_power_of_regulation/index.html#next-up-to-read",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "Next up to read",
    "text": "Next up to read\nThe series concludes and everything written about comes together in a series of innovative product summaries and ideas!"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html",
    "href": "posts/p5_energized_ai_hypotheses/index.html",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "",
    "text": "Explore conventional and innovative applications of AI in the energy sector"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#the-grid",
    "href": "posts/p5_energized_ai_hypotheses/index.html#the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "The grid",
    "text": "The grid\nSimilar to the internet, the nationals power grid operates as a vast network of transmission cables and routing substations to deliver electricity from producer to consumer. The three primary components include:\n\ngeneration\ntransmission (high-voltage lines between producers)\ndistribution (low-voltage lines to consumers)\n\n\nCongestion\nPhysical limitations in the grid mean the amount of power available in an area will always have a cap on it.\nOften times during certain times of a season or during major events, demands for electricity will exceed the capacity of the grid for a short time.\nA common example of this would be excess power generation in a low demand area with insufficient transmission capacity to move it to areas of high demand.\nOutages in production, or temporary maintenance of a line, will increase electrical load on remaining lines.\nBottlenecks will form in areas that contain multiple lines converging‚Äîleading to higher levels of energy congestion."
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#connecting-to-the-grid",
    "href": "posts/p5_energized_ai_hypotheses/index.html#connecting-to-the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "Connecting to the grid",
    "text": "Connecting to the grid\nWhen a new power generation facility, such as a wind farm or new hydroelectric dam, finishes production‚Äîtransmission lines will be connected from the new facilities step-up transformer (matching existing voltage and frequency) to the nearest substation.\nOnce hooked up to the grid, the new production facility will gradually increase power output until complete synchronization!"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#regulating-the-grid",
    "href": "posts/p5_energized_ai_hypotheses/index.html#regulating-the-grid",
    "title": "By the Power of Regulation - [PART 4]",
    "section": "Regulating the grid",
    "text": "Regulating the grid\nPower is power, so-to-speak, and everybody wants it! As a result, several large governing bodies have to be involved with any major power change:\n\nFederal Energy Regulatory Commission (FERC): an independent agency which regulates the interstate transmission of electricity and oversees wholesale electricity markets\n\nex: the FERC would approve a proposal to build new interstate transmission lines between Arizona and California\n\nNorth American Electric Reliability Corporation (NERC): A non-profit organization that develops and enforces reliability standards for the bulk power system in North America\n\nex: NERC developed and enforced a new cybersecurity standard for power grid operators after a series of attempted cyberattacks on utility companies.\n\nRegional Transmission Organizations (RTOs) and Independent System Operators (ISOs): non-profit organizations that coordinate, control, and monitor the electric grid across multiple states and helps manage the wholesale electricity markets to ensure reliable operation of the grid\n\nex: during heatwaves in the Midwest, the Midcontinent Independent System Operator (MISO) coordinates with power plants across several states to increase electricity generation and manage the grid to prevent blackouts\n\nState Public Utility Commissions (PUCs): state-level agencies that regulate the retail electricity markets and oversee the operations of utilities within their state\n\nex: The California Public Utilities Commission (CPUC) approved a rate increase for a major utility company to fund wildfire prevention measures\n\nLocal utilities: companies responsible for the final distribution of electricity to end-users and maintaining the local distribution infrastructure\n\nex: A municipal utility located within a small town in Oregon implemented a smart meter program for all its customers, allowing for more accurate billing\n\n\nThese governing bodies have created various frameworks to understand and control the market forces that dictate when new energy needs to be produced, how it will transfer there, and who will get to use it.\nRTOs and ISOs have algorithms and market mechanisms to optimize how much energy powers plant should be producing and the capacity of operation required to meet market demands.\n\nHow this maps to the great state of Idaho\nDue to its heavy reliance on hydroelectric power, Idaho‚Äôs energy landscape has a relatively simple regulatory framework compared to states with multiple overlapping RTOs or ISOs.\nHere‚Äôs how it breaks down:\n\nFERC: Issues licensing and oversees projects the Snake River as it is considered a major interstate water resource and therefore everything utilizing it must be built within compliance of federal regulation.\nNERC: Idaho falls under the Western Electricity Coordinating Council (WECC), which is the regional entity of NERC responsible for coordinating and promoting bulk electric system reliability in the Western Interconnection.\nRTO/ISO: Idaho does not belong to any Regional Transmission Organization (RTO) or Independent System Operator (ISO). Most of the state‚Äôs transmission system is operated by individual utilities\nPUC (Public Utilities Commission): Idaho has one state-level regulatory body‚Äî theIdaho Public Utilities Commission (IPUC)\nMajor Utilities in Idaho: While not regulatory bodies themselves, these are the main companies regulated by the IPUC:\n\nIdaho Power Company\nAvista Utilities\nRocky Mountain Power (a division of PacifiCorp)\nFall River Rural Electric Cooperative\nKootenai Electric Cooperative"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#next-up-to-read",
    "href": "posts/p5_energized_ai_hypotheses/index.html#next-up-to-read",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "Next up to read",
    "text": "Next up to read\nThe series has concluded but read up on my thoughts about what makes a good or bad AI business"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#exploring-ai-profit-margin-improvement-plays-in-the-power-sector",
    "href": "posts/p5_energized_ai_hypotheses/index.html#exploring-ai-profit-margin-improvement-plays-in-the-power-sector",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "Exploring AI profit margin improvement plays in the power sector",
    "text": "Exploring AI profit margin improvement plays in the power sector\nAt this point in the series, we‚Äôve covered:\n\na high level exploration of the energy sub-sectors\na deep dive into dams and the mechanics of hydroelectric energy generation\nthe mechanics of energy transmission and how the grid operates as whole\ngrid congestion occurs and the major regulation players involved with day-to-day power operations\n\nSo where does AI solutions and products fit into any of this?\nI think of AI products and solutions as fitting one of two buisness case archtypes:\n\nproducts and solutions that increase an existing business‚Äôs profit margins\nproducts and solutions that create a new capability and leads to previously impossible functionality and/or revenue streams\n\nI talk about my opinions on the two archtypes more in this post but TLDR I think the former sucks."
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#using-ai-to-innovate-and-create-new-capabilities-previously-unavailable",
    "href": "posts/p5_energized_ai_hypotheses/index.html#using-ai-to-innovate-and-create-new-capabilities-previously-unavailable",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "Using AI to innovate and create new capabilities previously unavailable",
    "text": "Using AI to innovate and create new capabilities previously unavailable\nI feel like most people agree that adding new capabilities and features with technologies tends to add the most value to the market‚Äîbut they also tend to be the hardest to build effectively.\nWith that in mind I thought it would be a good exercise to hypothesize some ways that AI-centric companies can be built to serve the energy sector.\n\nMicro-hydro opportunities\nPublic rivers such as the Idaho Snake River often run adjacent to or through private properties. I‚Äôm not sure of the exact legal ramifications, but individuals could leverage an AI-driven platform that helps them to identify and monetize micro-hydro opportunities.\nThe algorithms would essentially:\n\nanalyze various satellite and terrain data to identify potential micro-hydro sites\nconnect property owners with either equipment distributors or some other capital investor\nensure new energy produced would be optimally added to the existing grid\ncooperate with electricity market makers for a fair energy rate\n\nMight not be possible but sounds cool!\n\n\nAI-powered energy futures market\nThis one leans towards web3 crypto style thinking, but a futures market which enables individuals / business to hedge against future energy prices and availability could be super cool.\nIt would go something like this:\n\nalgorithms predict hyper-local energy prices and availability based on various factors\nanybody can ‚Äúbuy‚Äù future energy allocations at the price predicted during purchase time\nenergy allocations can be exercised\nall the typical derivatives associated with a futures market can be enabled for more complex trading processes\n\n\n\nMobile solar farm deployment swarms\nSolar panels have continued to get smaller, more efficient, and easier to use.\nAI drone swarm technology has also arrived at an impressive capability point.\nTherefore, the product combines these two concepts as such:\n\nalgorithms determine optimal locations for temporary solar energy capture\nmodular drone swarm of mobile solar panels moves to these locations on an efficient logistical route\nthis dynamic solar infrastructure maximizes renewable energy in areas previously unusable and can be moved to new areas easily during seasons of low energy generation\n\nI‚Äôm imagining some Call of Duty or James Bond villain level shenanigans with this idea but it could be a great way to rapidly expand the grid in high-demand areas during peak usage time periods without letting capital get sunk in seasonal usage areas.\n\n\nCybersecurity digital twin of the energy grid\nDigital twin companies have been having their hay day and it can apply to the energy grid as well.\nThe product would go something like this:\n\nAI maintained real-time digital twin of a state‚Äôs energy system in a sector-by-sector or state-wide view\nvarious cybersecurity war games and exercises could be performed on the digital twin without risk to the real world system\nAI agents simulate attacks to better understand threats and vulnerabilities\n\n\n\nHydroelectric edge computing network data centers\nThis one has a hardware component but a network of hydroelectric-powered edge devices distributed across public waterways could act as a new form of data center\n\nalgorithms determine optimal placement of small data centers near hydroelectric sources\nexcess hydroelectric capacity directly transfers to the facility and becomes a green compute power resource that can be sold as usual\nAI systems manage workload distribution based on real-time energy flucations\n\nMeets growing demands for sustainable cloud computing while leveraging hydroelectric oversupply in low-demand areas."
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#conclusion",
    "href": "posts/p5_energized_ai_hypotheses/index.html#conclusion",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "Conclusion",
    "text": "Conclusion\nThe energy sector operates as one of the largest market sectors in the world and has many ripe opportunities.\nSimple profit margin plays can be made galore, but with a bit of clever thinking AI can be used to create new niches previously impossible that lead to new capabilities."
  },
  {
    "objectID": "posts/p6_margin_plays_suck/index.html",
    "href": "posts/p6_margin_plays_suck/index.html",
    "title": "AI businesses built to be profit margin plays suck",
    "section": "",
    "text": "Understand how energy transmission works from a fundamental perspective\n\n\nI‚Äôve had three conversations in the last week with colleagues on the topic of ‚Äúgood AI business concepts‚Äù, and each one has more-or-less gone this way:\n\nconvo starts off with ‚ÄúI have an idea and I‚Äôm doing x, y, and z for my minimum viable product (MVP)‚Äù\nwe outline how the MVP will solve a business case and that several other companies have done similar things in addressable markets that can be used for effective comparison\nI talk about how in my personal experience this is going to probably suck to build and it‚Äôll suck even more to make it into a real cash cow\nwe agree that somebody out there can probably do this (so why not ‚Äúme‚Äù)\nconversation ends and I‚Äôm glad that I‚Äôm not the one that has to build that MVP\n\nThe tech industry has been 10x pilled for awhile now thanks to Peter Thiel and the main spin off derivatives. I agree 100% in the framework, and that the power law exists, but I have a bone to pick with building a company that exists in the ‚Äúwe improve X% and rake in Y% of that as a kickback‚Äù space.\nHistory has shown us that dozens of startups emerged in the last year built the idea of creating an AI product or service for an easy 10x on some critical part of an unsexy business vertical‚Äôs core offering and that companies can indeed be founded with solid venture capital.\nSo it‚Äôs not that AI profit margin plays can‚Äôt happen‚Äîit‚Äôs that they suck to build and I don‚Äôt want to be the one that does that."
  },
  {
    "objectID": "posts/p6_margin_plays_suck/index.html#non-ai-companies-using-ai-to-improve-margin-tend-to-be-big-winners",
    "href": "posts/p6_margin_plays_suck/index.html#non-ai-companies-using-ai-to-improve-margin-tend-to-be-big-winners",
    "title": "AI businesses built to be profit margin plays suck",
    "section": "Non-AI companies using AI to improve margin tend to be big winners",
    "text": "Non-AI companies using AI to improve margin tend to be big winners\nI want this opinion piece to be cut by some great examples of start-ups built or leveraging AI to capitalize on some margin improvement business case as a compliment to their primary business venture to demonstrate that it AI can and should be used to improve productivity:\n\nwatershed - layers in several different types of algorithms to further increase their decarbonization solution success rates\nveho - fundamentally a logistics company that employs AI to further push the improvements on their logistical solution offerings\ndeel - a payroll and compliance platform using AI to further improve the effectiveness of their compliance offerings"
  },
  {
    "objectID": "posts/p6_margin_plays_suck/index.html#ai-companies-built-to-improve-margin-tend-to-suck",
    "href": "posts/p6_margin_plays_suck/index.html#ai-companies-built-to-improve-margin-tend-to-suck",
    "title": "AI businesses built to be profit margin plays suck",
    "section": "AI companies built to improve margin tend to suck",
    "text": "AI companies built to improve margin tend to suck\n\nolive ai - attempted to automate and improve hospital related administrative tasks in an attempt to reduce costs‚Äîrising to a four billion dollar evaluation and then shutting down\nAtrium - 50% legal tech company, 50% AI innovation company, 100% closed down\nClarifai (improving accuracy in visual recognition), H20.ai (improving business processes), Zume (improving the food industry) - AI driven companies still around today, and could have potentially strong exits, but all had to go through some form of restructure with major lay offs üò¨\n\nThe point of this section being‚Äîthe space exists for a reason but it can be grueling to play in.\nTODO - some big AI winners and a blurb about deep tech is the play for making new features?"
  },
  {
    "objectID": "posts/p6_margin_plays_suck/index.html#next-up-to-read",
    "href": "posts/p6_margin_plays_suck/index.html#next-up-to-read",
    "title": "AI businesses built to be profit margin plays suck",
    "section": "Next up to read",
    "text": "Next up to read\nStay tuned for more!"
  },
  {
    "objectID": "posts/p5_energized_ai_hypotheses/index.html#using-ai-to-improve-profit-margins-for-solutions-that-currently-exist",
    "href": "posts/p5_energized_ai_hypotheses/index.html#using-ai-to-improve-profit-margins-for-solutions-that-currently-exist",
    "title": "Energized AI Hypotheses - [PART 5]",
    "section": "Using AI to improve profit margins for solutions that currently exist",
    "text": "Using AI to improve profit margins for solutions that currently exist\n\nPredictive solutions\nIn almost every aspect of the energy sector stack we see a need of predictive methods. Anything that requires maintenance could probably benefit from improved efficiency and a typical tech dashboard. I‚Äôm trying not to roll my eyes writing this one.\nAs new energy sources such as solar and wind become available, these new facilities will benefit from weather predictions impacting their generation capabilities and optimizing storage systems for fluctuations in generation windows.\nMost business cases would be along the lines of, ‚Äúimprove margins for X by increasing uptime, providing more stable power during peak hours, performing maintenance operations during downtime, stopping lost cash during unexpected downtime etc.‚Äù\nI would add water resource management type solutions that predict climate related variables such as water availability (snowpack + rainfall), reservoir management and irrigation, and whatever other long-term climate factors that impacts water production.\n\n\nSmarter distribution solutions\nThink anything that makes it more efficient to move or use energy in areas that need it at time of demand.\nBusiness case would be similar to predictive solutions in that users improve business margins by making sure existing capabilities get used to maximum performance.\n\n\nConsumer energy efficiency solutions\nAnything personalized or tailored to individual consumers that improves energy consumption based on their usage pattern will probably be an effective business case.\nBusiness case would be, ‚Äúyou save X% on your energy, feel good about being a smaller consumer, and we get some type of kickback‚Äù.\n\n\nCybersecurity improvement solutions\nEach of the regulation bodies has to deal with cybersecurity in some fashion. AI can be extremely powerful when it comes to detecting and responding to cyber threats‚Äîincluding vulnerability / penetration testing or simulation scenarios.\nAnother simple business case of, ‚Äúwe protect your shit so the American government doesn‚Äôt crash‚Äù\n\n\nRegulatory compliance and reporting solutions\nPeople don‚Äôt perform stare-and-compare actions well and AI has a great working knowledge of compliance databases.\nAnything that automates data collection or improves report generation accuracy will probably be successful.\nReal-time monitoring operations and prediction of potential compliance issues can be layered in for extra value.\nA more complex business case that can go towards improving margins through reduction of seasonal workers and/or time to compliance."
  }
]